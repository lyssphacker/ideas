### Sending objects, not data


#### [Seminar on Object Oriented Programming](https://www.youtube.com/watch?v=QjJaFG63Hlo); starts at 2:11:50
The original Smalltalk did not use kind of procedure call ... it really acted as you were on a network. ... When you run Smalltalk over a network people make connection to the network by having what are called phantom objects. Phantom object is a stand-in for the object you are sending message to ... cause you are always sending a message to something. .. [Further explanation is done whith [this](slides/sending-object.png) slide] ... Suppose you are here and you think you are sending a message from object A to object B. They are maybe on different machines, so what is maybe really going on is object A is sending to a version of object B which is really going over a network to real B. These are called phantom objects. There is usually one class of them, and whenever you are distributing computation, usually the environment for objects of few local objects and the rest of them are phantom objects. Phantom objects take care of all of the network stuff and synchronizations, and reorderings and other kinds of things you have to do when you are going to slower media. That is well established technique, and in any good object-oriented language you can set that up literary in an afternoon. It is quite instructive to think about what it means not to send just a message but to send an object from one place to another. What parts of it's environment we have to bring along with it in order for it to be understood? In theory, sending an object (say we send object C across) ... C might not be well understood on the other side. On the other hand, it is still active, so if your network has the rule, as ours did at PARC, that whenever an object shows up and it's class is not there, the clone of it's class gets sucked over with it. Compare that to sending a data structure, which is sending bits, and how fragile they are, because you are not sure there are procedures on the other end to know them. Here you are ensuring that either nothing at all happens or right procedures are automatically sent along. That is exactly how Air training command protected their tapes [reference to the story from the beginning of the presentation]. You just ensure that procedures goes along with them. And it is quite easy, using these phantom objects to make that automatic. So you can think of the network objects as constantly rebalancing themselves as to where the code actually is. 

---

#### [The computer revolution hasnt happened yet - OOPSLA 1997](https://www.youtube.com/watch?v=oKg1hTOQXoY); 21:24 - 26:05  
This is the earliest known form of what we call data abstraction. It goes all the way back to pre '61. I was in the Air Force in 1961 and I saw it then. It probably goes back one year before. Back then they did not really have operating systems. Air training command had to send tapes of many kinds of records around from air force base to air force base. There was a question how could you deal with all these that used to be card images, and then because tape had come in there are starting to be more and more complicated formats. And somebody, almost cerainly an enlisted man, because officers did not program back then, came up with the following [idea](slides/b220.png). This person said, on the third part of the record on this tape we will put all the records of this particular type. On the second part we will put all the procedures that know how to deal with the formats on this third part of the tape. And on the first part we are going to put pointers into the procedures and lets make first 10 or so pointers standard, like reading and writing fields, trying to print, lets have standard vocabulary for the first 10 of these and then we can have idiosyncratic ones later on. So all you have to do to read a tape back in 1961 is to read the front part of the record in core storage and start jumping indirect through the pointers and the procedures were there. Now, I would really like you to contrast that with what you have to do with HTML on the Internet. Think about it. HTML on the Internet has gone back to the dark ages because it presupposes that there should be a browser that should understand it's formats. That has to be one of the worse ideas since MS-DOS. This is really a shame. This is maybe what happens when physicists decide to play with computers, I am not sure. And in fact, we can see what has happened to the Internet now. There are 2 wars going on. There is set of browser wars which are 100% irrelevant. They are basically an attempt either at demonstrating a non-understanding of how to build complex systems, or even cruder attempt simply to gather teritory. I suspect Microsoft is in the latter camp here. You do not need a browser if you followed what this staff sergeant in the Air Force knew how to do in 1961. You just read it in, it should travel with all the things it needs, and you do not need anything more complex than something like X Windows, hopefully better. But, basically you want to be able to distribute all of the knowledge of these things that are there. And in fact the Internet is starting to move in that direction as people discover ever more complex HTML formats ever more intractable. This is one of these mistakes that has been recapitulated every generation, and it is just simply not the way to do it. So, here it is a great idea. BTW, this kind of programming was done before there were higher level languages in the Air Force, and this approach to things was forced out of the Air Force by Cobol, when they standardized on Cobol.  

---

#### [What if "data" is a really bad idea? - exchange between Alan Kay and Rich Hickey](https://news.ycombinator.com/item?id=11945722)  

**RH:** Data like that sentence? Or all of the other sentences in this chat? I find 'data' hard to consider a bad idea in and of itself, i.e. if data == information, records of things known/uttered at a point in time. Could you talk more about data being a bad idea?

**AK:**  What is "data" without an interpreter (and when we send "data" somewhere, how can we send it so its meaning is preserved?) 

**RH:** Data without an interpreter is certainly subject to (multiple) interpretation :) For instance, the implications of your sentence weren't clear to me, in spite of it being in English (evidently, not indicated otherwise). Some metadata indicated to me that you said it (should I trust that?), and when. But these seem to be questions of quality of representation/conveyance/provenance (agreed, important) rather than critiques of data as an idea. Yes, there is a notion of sufficiency ('42' isn't data).

Data is an old and fundamental idea. Machine interpretation of un- or under-structured data is fueling a ton of utility for society. None of the inputs to our sensory systems are accompanied by explanations of their meaning. Data - something given, seems the raw material of pretty much everything else interesting, and interpreters are secondary, and perhaps essentially, varied.

**AK:** There are lots of "old and fundamental" ideas that are not good anymore, if they ever were.

The point here is that you were able to find the interpreter of the sentence and ask a question, but the two were still separated. For important negotiations we don't send telegrams, we send ambassadors.

This is what objects are all about, and it continues to be amazing to me that the real necessities and practical necessities are still not at all understood. Bundling an interpreter for messages doesn't prevent the message from being submitted for other possible interpretations, but there simply has to be a process that can extract signal from noise.

This is particularly germane to your last paragraph. Please think especially hard about what you are taking for granted in your last sentence.

**RH:** Without the 'idea' of data we couldn't even have a conversation about what interpreters interpret. How could it be a "really bad" idea? Data needn't be accompanied by an interpreter. I'm not saying that interpreters are unimportant/uninteresting, but they are separate. Nor have I said or implied that data is inherently meaningful.

Take a stream of data from a seismometer. The seismometer might just record a stream of numbers. It might put them on a disk. Completely separate from that, some person or process, given the numbers and the provenance alone (these numbers are from a seismometer), might declare "there is an earthquake coming". But no object sent an "earthquake coming" "message". The seismometer doesn't "know" an earthquake is coming (nor does the earth, the source of the 'messages' it records), so it can't send a "message" incorporating that "meaning". There is no negotiation or direct connection between the source and the interpretation.

We will soon be drowning in a world of IoT sensors sending context-or-provenance-tagged but otherwise semantic-free data (necessarily, due to constraints, without accompanying interpreters) whose implications will only be determined by downstream statistical processing, aggregation etc, not semantic-rich messaging.

If you meant to convey "data alone makes for weak messages/ambassadors", well ok. But richer messages will just bottom out at more data (context metadata, semantic tagging, all more data) Ditto, as someone else said, any accompanying interpreter (e.g. bytecode? - more data needing interpretation/execution). Data remains a perfectly useful and more fundamental idea than "message". In any case, I thought we were talking about data, not objects. I don't think there is a conflict between these ideas.

**AK:** 2nd Paragraph: How do they know they are even bits? How do they know the bits are supposed to be numbers? What kind of numbers? Relating to what?

Etc

**RH:** It contravenes the common and historical use of the word 'data' to imply undifferentiated bits/scribbles. It means facts/observations/measurements/information and you must at least grant it sufficient formatting and metadata to satisfy that definition. The fact that most data requires some human involvement for interpretation (e.g. pointing the right program at the right data) in no way negates its utility (we've learned a lot about the universe by recording data and analyzing it over the centuries), even though it may be insufficient for some bootstrapping system you envision.

---

#### [Normal Considered Harmful](https://www.youtube.com/watch?v=FvmTSpJU-Xc); 16:02 - 27:00
related slides:  
[[1]](slides/reasonable-questions.png)[[2]](slides/16-years-ago.png)[[3]](slides/how-should-this-really-been-done.png)  
Some years ago my wife ... she had one of these huge Mac Cinema displays on her desk, and she showed me this display. It is kind of odd. Over here she had all the applications that she used were WYSIWYG. She really like them. Here are all the stuff she does on the web and none of it was WYSIWYG at that time. In these apps I can see and do full WYSIWYG authoring. Web broswer has all these modes, I have to type throught a keyhole, I have to wait to see whether what I did is ok. She asked me, why is that? I said, because the stuff she liked was invented in the 70s and the stuff she does not like was invented 20 years later. What do you think about that? How could that happen? It is not that people do not make mistakes. But you would like for mistakes to go away. Of course, where was this invented? Web browser was done by University of Illinois studens little bit after that age of people made Plato terminals and supercomputers.  
So 1993, here is Mosaic. One of the interesting things about Mosaic is that it was not remotely as nice as what Engelbart has done in '68. It had full documentation. Of course, they did not know anything about it. Tim Berners Lee, who did the Web and CERN was quite chagrined when it was pointed out to him just of suite of Englebart's ideas. He was really upset they did what they did and the way they did it. And even more interesing, 6 years before the Web browser there was Hypercard at Apple, which was the perfect model. Those of you who are familiar with Hypercard, it is a perfect model for what the web browser should actually be. It had full WYSIWYG editing right in the thing, and anybody who knew this at all would have realized, oh yes, this is a perfect thing, it already has hyperlinking in it, there are editing models, it has been tested on 4 million users, you know it works. No, a bunch of hackers got together. One of the ways of thinking about this is, Englebart invented the wheel, and Bill Atkinson with Hypercard invented the better wheel. That is really great. Unfortunately, I cannot even give the web browser the flat tire award, or even square wheel, because you can imagine improving both ot those to being something good, but when you invent the broken wheel, it is not obvious what the wheel is. It does not even work at all. Flat tire, you can run it in the expense of a tire. For square wheel, you can figure out it should be rounder. But the broken wheel won't work, and so attempts at fixing broken wheel produces more variations of the broken wheel, and we had 16 years of them.  
Suppose you had undergraduate degree in computing from back then, and you are faced with this idea that there is Internet that is going all over the world and it is going to be not just a consumption mechanism for stuff that is already there, but everybody is going to be an auther or publisher. So, how would you solve that problem? This is complicated problem. There are 6 billion people in the world. There are now couple of billion nodes on the Internet. Everybody wants to do something. How much do you think your browser should actually know in order to solve this problem really well? Everybody have different interest in the Web. There is many kinds of media. You do not even know what kind of media is going to be invented. For any given kind of media, like video, there are dozen codecs. ... Suppose they want to write machine code because they know how to do something really, really fast. Suppose somebody wants to implement their own codec. I am talking about letting people exercise their creativity. Sometimes they are going to need every resource on the machine to be used. Of course, you cannot allow them to take over the machine. Google has some real interest in this. Google did V8 because they could not stand to have normal conception of slow JavaScript. They are doing the thing just mentioned because they realize that people have ideas also. So, it really should be like an operating system kernel. Operating system kernel controls address spaces that can confine computations completely. Therefore you can allow any binaries to come down and be used. You completely control what goes in and what goes out. All of a sudden your browser is simply the thing that allows bitmaps canvases that you give to these things to write on, to be displayed. So, there is nothing you have to do. The good idea in Unix way, hey we do not want a big operating system. We want the tiniest kernel we can have and then we want to use address spaces to protect everybody from everybody else. So this is operating systems 101. This was well known all the way back to 1965 and 1966. It should be the first thing that occurs to anybody. Everybody in the 60s and 70s knew this is the way to do it and we were all surprised it was not done this way. I figured somebody in the current generation would have figured this out, so I went on the Net using Google, looking for downloadable binaries and stuff like that. And I found this. Grad student at Cornell in 1997 Ulfar Erllingson, who is from Iceland, wrote this paper. It is an HTML document that he put on the Net in 1997, and his problem was, hey I want to write a codec, and I do not want for system to be worried about whether it is an executable or not. So this thing should be like an operating system. So of course, I wrote an email to him. He is at the university of Iceland now, but on leave to Microsoft, up in Mountain View, and had a long correspondence with him on the pathway of these ideas. He started a company that Google aquired and is using a couple of the ideas but not all in this paper, which actually solved the problem really nicely. So this is an interesting thing. I was not that somebody did not understand what the problem was and what the right solution was, it is that the larger mass was quite happy to plunge into the defacto standard. Even though most of the people who used the web were computer scientists, they did not protest that this is very bad, non scalable way of doing it. They just plunged in, and 16 years later Google and other companies are struggling to use the Internet the way it was intended to be. 

---

#### Excerpts from [How could the web have been developed following Alan Kay's vision of a browser being a mini-operating system and not just an application?](https://www.quora.com/How-could-the-web-have-been-developed-following-Alan-Kays-vision-of-a-browser-being-a-mini-operating-system-and-not-just-an-application)  

A “real object” is a sandbox — this is one of the many things that are not understood about “real objects”. This is why I use the “OS” metaphor, because OSs confine processes to be sandboxed. The ability to do this at finer grains is made difficult because the CPU makers don’t understand what SW needs. This has led to the very bad idea of unprotected threads running inside the protected processes (what is needed is protection at every level of execution). A good OS is only about resource allocation (space and time and I/O, including display), and should be able to deal with processes without having to know what they are doing. This is what the web browser should do instead of the awful stuff it does do.  

When you were using Squeak you were using a system that gave you the programmer privilege to the metasystem. However, because of what OOP is (with real messaging etc), “fences” can be set up to protect dangerous actions via privilege, etc. One way to look at this is that each meta-level allows more fundamental power but is also more dangerous (even going from pure functions to procedures with side-effects is quite dangerous — even an assignment statement, even inside an object is a meta-level beyond functional relationships).

The point of “messages are not commands” is that something can decide whether to treat a message as a command (in general it shouldn’t). Right now much of the world is reeling because MS didn’t do a good job of this, NSA found out, apparently didn’t tell them right away, got themselves hacked, and now the bug is being exploited because many people didn’t apply the MS patch (and a lot of the reason is because of the pain of dealing with system updates).

Take a look at what your namesake (one of the other Mark Millers) has done with the capability idea. Part of going from “science” and “learning” to “engineering” is to create fences of various kinds. The important idea is that these don’t have to be imposed for all time and early like static languages. But they are needed.

And, yes, the reason we did “Worlds” (and Alex did a great job on this!) was to take a very old very good idea of McCarthy’s — simulating “time” — and try to make it more general and work at more scales. This was proposed for Smalltalk at Parc but it wasn’t pragmatically feasible back then. This is one of many reasons I say that Smalltalk is quite obsolete today — it doesn’t have the stuff that is needed that *can* be done today. Programming language should at least have a “half-life” so they will eventually disappear and force better versions.

---

#### [Exchange with Tony Li](https://www.quora.com/How-could-the-web-have-been-developed-following-Alan-Kays-vision-of-a-browser-being-a-mini-operating-system-and-not-just-an-application)  

**TI**: Imagine the browser instead as a Java virtual machine that you would download code into and execute. Not only could it present a user interface, but it could also do client side computation.

However, this is extremely badly broken from a security standpoint.

**AK**: Well, don’t use Java. Make an OS that can handle and balance intercommunicating fine-grained protected processes in any language.

**TI**: Ok, I’ll bite. Why? The browser was meant to be a ubiquitous display tool, not a means for the server to foist arbitrary computation onto the client.

All of the times when we’ve tried to hand off computation to be executed in a sandbox, we’ve found that the sandbox has holes. It’s a security nightmare.

**AK**: A display for what? If for more than simple static text, the media content could very well require local computation, not just to generate the graphics, but also to get reasonable frame-rates. If we take “computer media” seriously, we have to think about arbitrary computations regardless of how they might be distributed between local and global computation.

The reason I use “OS” as a metaphor and analogy is that mid-modern OSs (starting in the 60s) required some form of hardware confinement to keep processes from messing each other up (this is a real “sandbox”). Most of these are very clunky and intertwined with swapping notions that are now outmoded. However, please do look at the Burroughs B5000 from 1962 to see a fine grained protection and mobility architecture for a multiprocessor machine that was close to uncrashable.

This is why the old original definition definition of “objects” defined them as virtual computers as encapsulated as HW computers are on the Internet (since you are an Internet guru you will know that a message sent by TCP/IP cannot command a computer in any way — this was intentional in the design). When bad things happen as the result of a TCP message, it is the fault of badly designed SW by someone else (usually an incompetent computerist).

Even with the poor use of gazillions of gates in modern CPUs, it is possible to organize things to protect regions of memory with perfect encapsulation. The offspring of the first “capability” protection offered by the B5000 can be used to make system very secure. One of the main bugs in most systems is that the security is left to the last, rather than being the thing to start with.

**TI**: A very nice dream, but the modern instantiation of hypervisors has proven repeatedly that the walls of the sandbox are way, way too thin. I’d be happy to change my tune if folks were able to demonstrate something robust. I’m not holding my breath.

Until then the responsible amount of computation that can be done on the client has to be exceedingly limited and undoubtedly inadequate by your standards. Sorry to say this, but we’ve blown off security for far too long and it’s now come back to bite us. Until we fix it, we won’t be making significant forward progress.

#### [Exchange with Miles Fidelman](https://www.quora.com/How-could-the-web-have-been-developed-following-Alan-Kays-vision-of-a-browser-being-a-mini-operating-system-and-not-just-an-application)  

**MF**: HyperCard almost got us there. So did OpenDoc.

Then again, things get very complicated, very quickly. Ted Nelson’s original vision of hyperlinks was bi-directional - nice in theory (much harder for links to break) - but much harder in practice (a static reference is lot easier to implement).

I expect things would get very, very messy if every web page contained tons of actors - all communicating continuously across the net. The potential for deadly embraces, resource exhaustion, and all kinds of nasty effects is pretty huge (not to mention whole new classes of spam, viruses, DDoS attacks, and so forth).

(Mind you, the notion of web pages as “smart documents” is intriguing - and an area where I’ve done some funded r&d - and looking for more. It’s just not that easy. Object Linking and Embedding, across Microsoft Office documents is bad enough. Making that work across the Internet, interoperably, is a long way off.)  

**AK**: Actually, not really. Take a look at the Croquet system (a realization of David Reed’s 1978 MIT Thesis), and its current manifestation as Open Cobalt (both of these have entries in Wikipedia). The idea of “simulating time” that goes back to John McCarthy allows much more than the traditional approaches.

I think the biggest problem here is that most computerists put forth enormous efforts roughly at the first levels of learning that they’ve gotten to. This is equivalent to trying to do science with what the Alexandrian Greeks were able to do — impressive compared with no science, but not when compared to the 17th century onward, and especially the 20th and the 21st centuries.

Along with all the rather ad hoc poking around that has characterized our field, the exceptions that are “mathematical and scientific” have allowed very difficult things (like the Internet (not the web)) to be designed and work well.    

**MF**: Hi Alan!

It’s your vision and interview being cited, so yours would be the definitive answer. :-) But…

From a technical perspective, I should think that networked, hyperlinked, Hypercard stacks (or OpenDoc containers) look a lot more like combining browser, operating system, and an object-oriented environment into one, than Croquet. (Though, perhaps your vision was not of a “browser being a mini-operating system and not just an application” as represented by the OP.)

Now Croquet has a coolness factor, and Teatime is an interesting protocol - but then immersive environments have limited utility for most things, and DIS & HLA are a bit more mature for maintaining distributed environments. But that’s another discussion entirely.  

**AK**: Many people have been distracted by the immersive shared 3D environment that is one of the things built on top of the Reed ideas, but if you just forget about this for a second and think about what the underlying system is doing, you’ll start to see the significance of the ideas. It’s basically internet-wide transactions on distributed objects.

When some of us were at Apple, we urged the company to use Hypercard as a model for end-user symmetric authoring and using. But these are different — though important — compared to what needs to be done at the systems level, and in particular to allow objects to freely be mixed to make media.  

**MF**: Now that’s a good point. It is a bit of a shame that the focus remained on the 3D environment. It does seem a bit of a shame that nobody ever pushed Teatime as a protocol, complete with RFC, multiple implementations, etc.

Then again, for whatever reason, this seems like an idea that’s never gotten traction outside of specialized arenas. Sun pushed “distributed objects everwhere,” Taligent never got anywhere, etc. DIS & HLA are pretty mature at this point, which do deal with distributed objects - and both are standardized - but no real traction outside the military training community (you’d think that HLA would be a great platform for MMORPGs).

Maybe it’s time for another push.  

**AK**: Dave Reed’s (and Croquet’s) schemes are much simpler and quite a bit more general: they are about the larger idea of “single logical objects” being able to manifest as “complete ghosts” on as many computers as needed over large slow networks. This is like atomic transactions on distributed databases but for objects and much finer grained in both space and time.  

And you are right that addressing these issues is very long overdue.

**MF**: Well, that is what DIS & HLA do - they’re all about synchronized replicated world models (every simulator has it’s own copy of the world). But I’m going to have to revisit Teatime in relation to some work I’m currently doing on replicated/synchronized documents.  

**AK**: Take a look at Reed’s 1978 MIT thesis for the gist.

**MF**: Thanks! Actually, I have read the papers, and a lot of documentation. (And talked with David about it, on occaision :-) I do need to refresh myself, though.

---

#### Excerpts from [What are the difficulties of creating "smart APIs", where two programs can automatically figure out a way to talk to each other?](https://www.quora.com/What-are-the-difficulties-of-creating-smart-APIs-where-two-programs-can-automatically-figure-out-a-way-to-talk-to-each-other)   

Also within this community [ARPA/PARC] were ideas about "software computers connected by neutral messaging" that could be a universal scalable way to unify -- and make mobile -- software systems, even with heterogeneous software systems (using similar encapsulation ideas and separating protocols from methods) that could freely be mapped onto the interconnected computers that the Internet would make possible. For example, Smalltalk at Xerox Parc in the 70s experimented with "proxy objects" which were gateways to objects in other systems and computers.

The really good idea of "the Internet as a transparent cache for "intercommunicating object-processes" never gained wide popularity (but see Reed's thesis below).

Also at Xerox Parc during the same period, it was realized that it was easier to send a program to a server to be "executed in confinement" than to try to get all servers to understand all data formats (this is where Postscript emerged from a graphics making language to a "traveling communications protocol"). The insight is that a rather simple executable language can be made universal, but that there is no end to possible formats of "data". This fits with the overall reasons for "send objects, not data".

Of great note is the [1978 MIT PhD thesis by Dave Reed](http://publications.csail.mit.edu/lcs/pubs/pdf/MIT-LCS-TR-205.pdf) -- who is also known as "the 'slash' in TCP/IP" -- which sketches an architecture for an Internet-wide operating system of coordinated objects running in a common pseudo-time. (These ideas were later validated in the [Croquet](https://www.researchgate.net/publication/4029258_Croquet_-_A_collaboration_system_architecture) System ca 2003 by Smith, Raab, Reed, and Kay).

The first great practical demonstration of "dynamic mobile processes" across heterogeneous hardware was done in ["The LOCUS Operating System"](https://mitpress.mit.edu/books/locus-distributed-system-architecture) by Gerry Popek and his group at UCLA during the early 80s. This wasn't extended beyond underlying Unix processes, but could have been. The first several chapters of the MIT Press book are excellent in laying out the general issues for machine independent processes.

There are also some "unfortunate" stabs at universal interfaces that pretty much missed the point of scaling, the Internet, and heterogeneity of software -- for example: [CORBA](https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture).
