### Computing is in a real trouble

#### Gerry Sussman, [Computer Science is in deep trouble](http://worrydream.com/refs/Sussman%20-%20Robust%20Design%20through%20Diversity.pdf)  
Computer Science is in deep trouble. Structured design is a
failure. Systems, as currently engineered, are brittle and fragile.
They cannot be easily adapted to new situations. Small changes in
requirements entail large changes in the structure and configuration.
Small errors in the programs that prescribe the behavior of the system
can lead to large errors in the desired behavior. Indeed, current
computational systems are unreasonably dependent on the correctness of
the implementation, and they cannot be easily modified to account for
errors in the design, errors in the specifications, or the inevitable
evolution of the requirements for which the design was commissioned.
(Just imagine what happens if you cut a random wire in your computer!)
This problem is structural. This is not a complexity problem. It
will not be solved by some form of modularity. We need new ideas. We
need a new set of engineering principles that can be applied to
effectively build flexible, robust, evolvable, and efficient systems.

#### Gerry Sussman, [We Really Don't Know How to Compute!](https://github.com/lyssphacker/talks/blob/master/we-really-do-not-know-how-to-compute/we-really-do-not-know-how-to-compute.md)  
We are in a real trouble and have not the foggiest idea how to compute very well. There might be some glimmer of hope on the horizon, in number of different directions, some of which I might point out.

#### Alan Kay, [The Present Does Not Compute](https://www.youtube.com/watch?v=tp9VbtLn2Jw&t=16m1s)
We are in the situation where the hardware capacity allows a lot of stuff that does not scale very well to barely survive, but survives at the expense of being able to improve it, make changes, understand it, etc.

#### Gerry Sussman, [Computer science actively discourages the construction of robust systems](https://groups.csail.mit.edu/mac/users/gjs/6.945/readings/robust-systems.pdf)
Observations of biological systems tell us a great deal about
how to make robust and evolvable systems. Techniques originally
developed in support of symbolic Artificial Intelligence can
be viewed as ways of enhancing robustness and evolvability in
programs and other engineered systems. By contrast, common
practice of computer science actively discourages the construction
of robust systems.

#### Alan Kay, [Most code looks like a garbage dump](https://www.youtube.com/watch?v=tp9VbtLn2Jw&t=7m40s)
Code is not organized nearly as nicely as a book. It looks more like garbage dump. It is the kind of thing that would be condemned if anybody could ever look at it, but if you look at garbage dump through a window about this big, it only looks slightly messy. If you could look at the entire garbage dump I think we would be horrified. It is not really a dump, because people have to live with this. So perhaps, this is a city after a great disaster where people are still forced to live.

#### Alan Kay, [General practice today is bellow any kind of minimum](https://www.youtube.com/watch?v=tp9VbtLn2Jw&t=44m10s)
(paraphrasing) If the computing does not reach higher qualitative bar in the next 10 years ... it will just be gone, and what we'll have is enormous ad-hoc mechanisms, in one of the longest, slowest crashes in history.

#### Alan Kay, [Software is slum-like](https://www.youtube.com/watch?v=QboI_1WJUlM&t=17m44s)
Picture that fits our code is a slum, particularly the web, but basically it is all rather slum-like. There is no large sense of architecture of any kind, things are tacked in there, urban renewal bulldozes some stuff out, but just leaves some stuff lying around.

#### Alan Kay, [Is “Software Engineering” an Oxymoron?](https://pdfs.semanticscholar.org/6c8d/f2c143c93290bcaa8338992a14aa7ac3369a.pdf)
If software does “engineering” at all, it is too often at the same level as the
ancient Egyptians before the invention of the arch (literally before the making of arches: architecture), who made
large structures with hundreds of thousands of slaves toiling for decades to pile stone upon stone: they used weak
ideas and weak tools, pretty much like most software development today.

#### Alan Kay, [Computing is a pop culture](https://queue.acm.org/detail.cfm?id=1039523)
You could think of it as putting a low-pass filter on some of the good ideas from the ’60s and ’70s, as computing spread out much, much faster than educating unsophisticated people can happen. In the last 25 years or so, we actually got something like a pop culture, similar to what happened when television came on the scene and some of its inventors thought it would be a way of getting Shakespeare to the masses. But they forgot that you have to be more sophisticated and have more perspective to understand Shakespeare. What television was able to do was to capture people as they were.

So I think the lack of a real computer science today, and the lack of real software engineering today, is partly due to this pop culture.

#### Alan Kay, [The Web - one of the worst things ever](http://www.youtube.com/watch?v=N9c7_8Gp7gI&t=28m30s)
The Web - I will not go into my diatribe about the web, but the web browser is one of the worst things ever. It only does a few things compared to what it should be doing. This is hard to explain to people. It is hard to find people who are willing to criticize the web.

#### Alan Kay, [GNU/Linux - budget of bad ideas](https://www.youtube.com/watch?v=rmsIZUuBoQs)
... learning Linux. It has thousands and thousands of commands. You can spend your entire life just ticking around with Linux and never learn anything about computing. It has the illusion of being something important about computing, but in fact it is kind of a budget of bad ideas.

#### Alan Kay, [Computing is getting more and more mundane](https://www.youtube.com/watch?v=1e8VZlPBx_0&t=54m58s)
As computing gets less and less interesting, it's way of accepting and rejecting things gets more and more mundane. This why some of these early systems ... Sam [Altman] looked at Sketchpad and asked: "Why aren't they doing it today?". Well, because nobody even thinks that this is important. Nobody even thinks of doing WYSIWYG on Web media. I just typed in some answers to Quora, and I was in the regime that was pre-70s. I was typing into a little window, I could not see how it was gonna look like until I clicked the button ... come on, this is bullshit. But nobody is protesting except old fogies like me because I know it can be better, you need to find out that it can be better.

#### Alan Kay, [Personal computing became a vast narcissistic mirror](https://www.youtube.com/watch?v=aHr4wAiXQD4&t=6m45s)
That by and large is what happened to personal computing since it came out commerically in early 80s. It basically became vast narcissistic mirror for the largest proportion of users, worse than almost any kind of thing we could have invented. We invented a new way for people to admire themselves and ignore almost anything of consequence.

#### Marvin Minsky, [AI has been brain dead since the 1970s](https://www.wired.com/2003/08/why-a-i-is-brain-dead/)
If AI's brain-dead, aren't you partially to blame?
No. I've been continually working on the problem. I'm trying to put a new project together, but it's hard to get 10 capable people. It would take five or ten years, and nobody wants to put that kind of time in - people want to double their money overnight.

#### Alan Kay, [iPad UI is very poor in a myriad of ways](http://techland.time.com/2013/04/02/an-interview-with-computing-pioneer-alan-kay/)
There is the desire of a consumer society to have no learning curves. This tends to result in very dumbed-down products that are easy to get started on, but are generally worthless and/or debilitating. We can contrast this with technologies that do have learning curves, but pay off well and allow users to become experts (for example, musical instruments, writing, bicycles, etc. and to a lesser extent automobiles).

#### Alan Kay, [Complications are 100-1000s of times larger than complexity](https://www.youtube.com/watch?v=ubaX1Smg6pY&t=6m44s)
There are 2 terms here. Complications, sometimes called accidental complexity, people trying to make it look better. Actually a lot of complications we put into code is not really that accidental, it comes from bad process. Complexity is some intrinsic measure of what the meaning of the code is. It is really hard to define ... We would like complications to be very close to the intrinsic complexity, and what we've got where the amount of complications can be hunderd, maybe thousands of times more.

#### Alan Kay, [Enourmous amount of noise and artifacts from the middle](http://www.youtube.com/watch?v=YyIQKBzIuBY&t=31m30s)
If we are going to take all of the people in our field and look at the super bright there are more than they were 40-50 years ago, but the relative number is much worse because the total number increased tremendeously and the absolute number of super bright has not increased commensurately. So there is enormous amount of noise from the middle today in the field.

#### Tommi Mikkonen and Antero Taivalsaari, [Web Applications – Spaghetti Code for the 21st Century](http://www.lively-kernel.org/repository/lively-kernel/trunk/doc/papers/SERA2008/SERA2008-Spaghetti-FINAL.pdf)
The software industry is currently in the middle of a
paradigm shift. Applications are increasingly written
for the World Wide Web. Unfortunately, the
technologies used for web application development
today violate well-known software engineering
principles, and they have reintroduced problems that
had already been eliminated years ago in the aftermath
of the “spaghetti code wars” of the 1970s. In this
paper, we investigate web application development
from the viewpoint of software engineering principles.
We argue that current web technologies are
inadequate in supporting many of these principles, but
also that there is no fundamental reason for web
applications to be any worse than conventional
applications in any of these areas. Rather, the current
inadequacies are an accidental consequence of the
poor conceptual and technological foundation of the
web development technologies today. 

#### Stuart Halloway, [The Impedance Mismatch is Our Fault](https://www.infoq.com/presentations/Impedance-Mismatch)
Impedance mismatch is a problem of our own making, and we invented it because both object-orientation and relational model are hoplessly complected.

#### Howard Shrobe, [Today's modern operating systems with nearly ~~50~~ 150 million source lines of code are neither visible nor controllable](https://dspace.mit.edu/handle/1721.1/7286)
Traditionally, we've focussed on the question of how to make a system easy to code the first time, or perhaps on how to ease the system's continued evolution. But if we look at life cycle costs, then we must conclude that the important question is how to make a system easy to operate. To do this we need to make it easy for the operators to see what's going on and to then manipulate the system so that it does what it is supposed to. This is a radically different criterion for success. What makes a computer system visible and controllable? This is a difficult question, but it's clear that today's modern operating systems with nearly 50 million source lines of code are neither. Strikingly, the MIT Lisp Machine and its commercial successors provided almost the same functionality as today's mainstream sytsems, but with only 1 Million lines of code. This paper is a retrospective examination of the features of the Lisp Machine hardware and software system. Our key claim is that by building the Object Abstraction into the lowest tiers of the system, great synergy and clarity were obtained. It is our hope that this is a lesson that can impact tomorrow's designs. We also speculate on how the spirit of the Lisp Machine could be extended to include a comprehensive access control model and how new layers of abstraction could further enrich this model.

#### Alan Kay, [Level of retrograde here is really back before 1965](http://www.youtube.com/watch?v=PFc379hu--8&t=46m33s)
The last time I looked at the task manager on my Windows OS I only saw 30 processes running. I would expect to see ... your little laptop there is equivalent of the entire Internet of the 80s sometime (computing, storage, processing power). When I look inside it I would expect to see a few thousand objects with no center, not tied to any particular operating system (why should they be?). Lets take Internet seriously for just one second. I love talking about it cause I almost have nothing to do with it. I have something to do with ARPANET, but the Internet was done by colleagues of ours, so it is not like claiming that Smalltalk is better than Java, which would be a bad thing to do. Since I have nothing to do with it I can praise it to the skies. Lets just look at it abstractly for a second. It is maybe the only real object system on Earth, where each object is entire computer (there is nothing more encapsulated than a piece of hardware), and pure messages are sent to it, really pure. One of the most interesting things about the Internet is that if an object that receives a message crashes as a result of that message, it is it's own fault, because we are not sending 100 thousand volts over the line there, we are just sending bits. Those bits are entirely interpreted by the programs on that object and the object gets to decide what to do. Suppose we can do that efficiently in smaller things (these are the thoughts I had in 1966). So the thing I love about the Internet is ... if I need something, there might be some object outhere that can supply it and it might be able to be of service to me, I do not care about the operating system it is running on or any other thing, because it is a pure messaging system. So, I am maybe doing one kind of thing, and that guy over there is doing maybe some other thing, I do not care. Next thing I want to do is to somehow get all those objects into my machine. So, the first thing I would talk to our friends at Intel about is why do not you understand virtual machines. The most important thing you can do on the computer today is confinment. So lets use some of that memory that you are using badly by doing caching wrong. Lets use it so that we can have thousands and thousands of virtual address spaces completely inexpensive. If you do math on it, it is absolutely possible. It is just not architecturaly in Intel's mind. Nobody had really explained to them why is this a good idea. And why do we not make interprocess messaging really fast, instead really slow the way Intel does it? And all of a sudden you start thinking ... I do not need an operating system, because what I really want are facilities (services). I must have confinment because things can be dangerous, but I do not want to rely on software for that - I want real confinment. I want to be able to get anything from Timbuktu that looks reasonable and chuck it into a virtual address space and confine the shit out of it. I want to do experiments with that thing. I want to feed it things and see what it feeds back to me. I want to use it in various guarded ways, etc. These are not new ideas, but they are like magic if you try to explain it to Intel, because Intel could not even to get to the point of allowing programmers to microcode so you can do higher-level languages. Level of retrograde here is really back before 1965, but it helps to be in computing before 1965 to see it, cause today for most people it looks normal.

#### Tony Li (in conversation with Alan Kay), [We’ve blown off security for far too long and it’s now come back to bite us](https://www.quora.com/How-could-the-web-have-been-developed-following-Alan-Kays-vision-of-a-browser-being-a-mini-operating-system-and-not-just-an-application)
TI: Imagine the browser instead as a Java virtual machine that you would download code into and execute. Not only could it present a user interface, but it could also do client side computation.

However, this is extremely badly broken from a security standpoint.

AK: Well, don’t use Java. Make an OS that can handle and balance intercommunicating fine-grained protected processes in any language.

TI: Ok, I’ll bite. Why? The browser was meant to be a ubiquitous display tool, not a means for the server to foist arbitrary computation onto the client.

All of the times when we’ve tried to hand off computation to be executed in a sandbox, we’ve found that the sandbox has holes. It’s a security nightmare.

AK: A display for what? If for more than simple static text, the media content could very well require local computation, not just to generate the graphics, but also to get reasonable frame-rates. If we take “computer media” seriously, we have to think about arbitrary computations regardless of how they might be distributed between local and global computation.

The reason I use “OS” as a metaphor and analogy is that mid-modern OSs (starting in the 60s) required some form of hardware confinement to keep processes from messing each other up (this is a real “sandbox”). Most of these are very clunky and intertwined with swapping notions that are now outmoded. However, please do look at the Burroughs B5000 from 1962 to see a fine grained protection and mobility architecture for a multiprocessor machine that was close to uncrashable.

This is why the old original definition of “objects” defined them as virtual computers as encapsulated as HW computers are on the Internet (since you are an Internet guru you will know that a message sent by TCP/IP cannot command a computer in any way — this was intentional in the design). When bad things happen as the result of a TCP message, it is the fault of badly designed SW by someone else (usually an incompetent computerist).

Even with the poor use of gazillions of gates in modern CPUs, it is possible to organize things to protect regions of memory with perfect encapsulation. The offspring of the first “capability” protection offered by the B5000 can be used to make system very secure. One of the main bugs in most systems is that the security is left to the last, rather than being the thing to start with.

TI: A very nice dream, but the modern instantiation of hypervisors has proven repeatedly that the walls of the sandbox are way, way too thin. I’d be happy to change my tune if folks were able to demonstrate something robust. I’m not holding my breath.

Until then the responsible amount of computation that can be done on the client has to be exceedingly limited and undoubtedly inadequate by your standards. Sorry to say this, but we’ve blown off security for far too long and it’s now come back to bite us. Until we fix it, we won’t be making significant forward progress.

#### Gerry Sussman, [People worry about security. Well they have no idea how to handle security](https://github.com/lyssphacker/talks/blob/master/we-really-do-not-know-how-to-compute/we-really-do-not-know-how-to-compute.md)
People worry about security. Well they have no idea how to handle security. We will find out for example that humans manage to survive for about 70 years. We are continuously attacked by the parasites. It must be that there are some bad things we are doing. It is all in that GB of code (human genome). You have to think about that. We certainly have no good ideas how to make things last for very long time by continuously being attacked by mutating parasites.

#### Alan Kay, [Nothing could be stupider than programmers trying to make Intel CPU look good](https://www.youtube.com/watch?v=ubaX1Smg6pY&t=9m3s)
It is true that we were able to build our own computers. That makes a huge difference because we did not have to do the kinds of optimizations that people do today, because people today got things backwards. Today we let Intel make processors that may or may not be good for anything and then programmers job is to make Intel look good by making code that will actually somehow run on it, and if you think about it, it could not be stupider. It is completely backwards. What you really want to do is to define your software first in a way that makes it most runnable, comprehensible, and then you want to be able to build whatever hardware is needed to run that software.
