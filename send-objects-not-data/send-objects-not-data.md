### Send objects, not data


#### [Seminar on Object Oriented Programming](https://www.youtube.com/watch?v=QjJaFG63Hlo); starts at 2:11:50
The original Smalltalk did not use kind of procedure call ... it really acted as you were on a network. ... When you run Smalltalk over a network people make connection to the network by having what are called phantom objects. Phantom object is a stand-in for the object you are sending message to ... cause you are always sending a message to something. .. [Further explanation is done with [this](slides/sending-object.png) slide] ... Suppose you are here and you think you are sending a message from object A to object B. They are maybe on different machines, so what is maybe really going on is object A is sending to a version of object B which is really going over a network to real B. These are called phantom objects. There is usually one class of them, and whenever you are distributing computation, usually the environment for objects of few local objects and the rest of them are phantom objects. Phantom objects take care of all of the network stuff and synchronizations, and reorderings and other kinds of things you have to do when you are going to slower media. That is well established technique, and in any good object-oriented language you can set that up literary in an afternoon. It is quite instructive to think about what it means not to send just a message but to send an object from one place to another. What parts of it's environment we have to bring along with it in order for it to be understood? In theory, sending an object (say we send object C across) ... C might not be well understood on the other side. On the other hand, it is still active, so if your network has the rule, as ours did at PARC, that whenever an object shows up and it's class is not there, the clone of it's class gets sucked over with it. Compare that to sending a data structure, which is sending bits, and how fragile they are, because you are not sure there are procedures on the other end to know them. Here you are ensuring that either nothing at all happens or right procedures are automatically sent along. That is exactly how Air training command protected their tapes [reference to the story from the beginning of the presentation]. You just ensure that procedures goes along with them. And it is quite easy, using these phantom objects to make that automatic. So you can think of the network objects as constantly rebalancing themselves as to where the code actually is. 

---

#### [The computer revolution hasnt happened yet - OOPSLA 1997](https://www.youtube.com/watch?v=oKg1hTOQXoY); 21:24 - 26:05  
This is the earliest known form of what we call data abstraction. It goes all the way back to pre '61. I was in the Air Force in 1961 and I saw it then. It probably goes back one year before. Back then they did not really have operating systems. Air training command had to send tapes of many kinds of records around from air force base to air force base. There was a question how could you deal with all these that used to be card images, and then because tape had come in there are starting to be more and more complicated formats. And somebody, almost cerainly an enlisted man, because officers did not program back then, came up with the following [idea](slides/b220.png). This person said, on the third part of the record on this tape we will put all the records of this particular type. On the second part we will put all the procedures that know how to deal with the formats on this third part of the tape. And on the first part we are going to put pointers into the procedures and lets make first 10 or so pointers standard, like reading and writing fields, trying to print, lets have standard vocabulary for the first 10 of these and then we can have idiosyncratic ones later on. So all you have to do to read a tape back in 1961 is to read the front part of the record in core storage and start jumping indirect through the pointers and the procedures were there. Now, I would really like you to contrast that with what you have to do with HTML on the Internet. Think about it. HTML on the Internet has gone back to the dark ages because it presupposes that there should be a browser that should understand it's formats. That has to be one of the worse ideas since MS-DOS. This is really a shame. This is maybe what happens when physicists decide to play with computers, I am not sure. And in fact, we can see what has happened to the Internet now. There are 2 wars going on. There is set of browser wars which are 100% irrelevant. They are basically an attempt either at demonstrating a non-understanding of how to build complex systems, or even cruder attempt simply to gather teritory. I suspect Microsoft is in the latter camp here. You do not need a browser if you followed what this staff sergeant in the Air Force knew how to do in 1961. You just read it in, it should travel with all the things it needs, and you do not need anything more complex than something like X Windows, hopefully better. But, basically you want to be able to distribute all of the knowledge of these things that are there. And in fact the Internet is starting to move in that direction as people discover ever more complex HTML formats ever more intractable. This is one of these mistakes that has been recapitulated every generation, and it is just simply not the way to do it. So, here it is a great idea. BTW, this kind of programming was done before there were higher level languages in the Air Force, and this approach to things was forced out of the Air Force by Cobol, when they standardized on Cobol.  

---

#### [What if "data" is a really bad idea? - exchange between Alan Kay and Rich Hickey](https://news.ycombinator.com/item?id=11945722)  

**RH:** Data like that sentence? Or all of the other sentences in this chat? I find 'data' hard to consider a bad idea in and of itself, i.e. if data == information, records of things known/uttered at a point in time. Could you talk more about data being a bad idea?

**AK:**  What is "data" without an interpreter (and when we send "data" somewhere, how can we send it so its meaning is preserved?) 

**RH:** Data without an interpreter is certainly subject to (multiple) interpretation :) For instance, the implications of your sentence weren't clear to me, in spite of it being in English (evidently, not indicated otherwise). Some metadata indicated to me that you said it (should I trust that?), and when. But these seem to be questions of quality of representation/conveyance/provenance (agreed, important) rather than critiques of data as an idea. Yes, there is a notion of sufficiency ('42' isn't data).

Data is an old and fundamental idea. Machine interpretation of un- or under-structured data is fueling a ton of utility for society. None of the inputs to our sensory systems are accompanied by explanations of their meaning. Data - something given, seems the raw material of pretty much everything else interesting, and interpreters are secondary, and perhaps essentially, varied.

**AK:** There are lots of "old and fundamental" ideas that are not good anymore, if they ever were.

The point here is that you were able to find the interpreter of the sentence and ask a question, but the two were still separated. For important negotiations we don't send telegrams, we send ambassadors.

This is what objects are all about, and it continues to be amazing to me that the real necessities and practical necessities are still not at all understood. Bundling an interpreter for messages doesn't prevent the message from being submitted for other possible interpretations, but there simply has to be a process that can extract signal from noise.

This is particularly germane to your last paragraph. Please think especially hard about what you are taking for granted in your last sentence.

**RH:** Without the 'idea' of data we couldn't even have a conversation about what interpreters interpret. How could it be a "really bad" idea? Data needn't be accompanied by an interpreter. I'm not saying that interpreters are unimportant/uninteresting, but they are separate. Nor have I said or implied that data is inherently meaningful.

Take a stream of data from a seismometer. The seismometer might just record a stream of numbers. It might put them on a disk. Completely separate from that, some person or process, given the numbers and the provenance alone (these numbers are from a seismometer), might declare "there is an earthquake coming". But no object sent an "earthquake coming" "message". The seismometer doesn't "know" an earthquake is coming (nor does the earth, the source of the 'messages' it records), so it can't send a "message" incorporating that "meaning". There is no negotiation or direct connection between the source and the interpretation.

We will soon be drowning in a world of IoT sensors sending context-or-provenance-tagged but otherwise semantic-free data (necessarily, due to constraints, without accompanying interpreters) whose implications will only be determined by downstream statistical processing, aggregation etc, not semantic-rich messaging.

If you meant to convey "data alone makes for weak messages/ambassadors", well ok. But richer messages will just bottom out at more data (context metadata, semantic tagging, all more data) Ditto, as someone else said, any accompanying interpreter (e.g. bytecode? - more data needing interpretation/execution). Data remains a perfectly useful and more fundamental idea than "message". In any case, I thought we were talking about data, not objects. I don't think there is a conflict between these ideas.

**AK:** 2nd Paragraph: How do they know they are even bits? How do they know the bits are supposed to be numbers? What kind of numbers? Relating to what?

Etc

**RH:** It contravenes the common and historical use of the word 'data' to imply undifferentiated bits/scribbles. It means facts/observations/measurements/information and you must at least grant it sufficient formatting and metadata to satisfy that definition. The fact that most data requires some human involvement for interpretation (e.g. pointing the right program at the right data) in no way negates its utility (we've learned a lot about the universe by recording data and analyzing it over the centuries), even though it may be insufficient for some bootstrapping system you envision.

---

#### [Normal Considered Harmful](https://www.youtube.com/watch?v=FvmTSpJU-Xc); 16:02 - 27:00
related slides:  
[[1]](slides/reasonable-questions.png)[[2]](slides/16-years-ago.png)[[3]](slides/how-should-this-really-been-done.png)  
Some years ago my wife ... she had one of these huge Mac Cinema displays on her desk, and she showed me this display. It is kind of odd. Over here she had all the applications that she used were WYSIWYG. She really like them. Here are all the stuff she does on the web and none of it was WYSIWYG at that time. In these apps I can see and do full WYSIWYG authoring. Web broswer has all these modes, I have to type throught a keyhole, I have to wait to see whether what I did is ok. She asked me, why is that? I said, because the stuff she liked was invented in the 70s and the stuff she does not like was invented 20 years later. What do you think about that? How could that happen? It is not that people do not make mistakes. But you would like for mistakes to go away. Of course, where was this invented? Web browser was done by University of Illinois studens little bit after that age of people made Plato terminals and supercomputers.  
So 1993, here is Mosaic. One of the interesting things about Mosaic is that it was not remotely as nice as what Engelbart has done in '68. It had full documentation. Of course, they did not know anything about it. Tim Berners Lee, who did the Web and CERN was quite chagrined when it was pointed out to him just of suite of Englebart's ideas. He was really upset they did what they did and the way they did it. And even more interesing, 6 years before the Web browser there was Hypercard at Apple, which was the perfect model. Those of you who are familiar with Hypercard, it is a perfect model for what the web browser should actually be. It had full WYSIWYG editing right in the thing, and anybody who knew this at all would have realized, oh yes, this is a perfect thing, it already has hyperlinking in it, there are editing models, it has been tested on 4 million users, you know it works. No, a bunch of hackers got together. One of the ways of thinking about this is, Englebart invented the wheel, and Bill Atkinson with Hypercard invented the better wheel. That is really great. Unfortunately, I cannot even give the web browser the flat tire award, or even square wheel, because you can imagine improving both ot those to being something good, but when you invent the broken wheel, it is not obvious what the wheel is. It does not even work at all. Flat tire, you can run it in the expense of a tire. For square wheel, you can figure out it should be rounder. But the broken wheel won't work, and so attempts at fixing broken wheel produces more variations of the broken wheel, and we had 16 years of them.  
Suppose you had undergraduate degree in computing from back then, and you are faced with this idea that there is Internet that is going all over the world and it is going to be not just a consumption mechanism for stuff that is already there, but everybody is going to be an auther or publisher. So, how would you solve that problem? This is complicated problem. There are 6 billion people in the world. There are now couple of billion nodes on the Internet. Everybody wants to do something. How much do you think your browser should actually know in order to solve this problem really well? Everybody have different interest in the Web. There is many kinds of media. You do not even know what kind of media is going to be invented. For any given kind of media, like video, there are dozen codecs. ... Suppose they want to write machine code because they know how to do something really, really fast. Suppose somebody wants to implement their own codec. I am talking about letting people exercise their creativity. Sometimes they are going to need every resource on the machine to be used. Of course, you cannot allow them to take over the machine. Google has some real interest in this. Google did V8 because they could not stand to have normal conception of slow JavaScript. They are doing the thing just mentioned because they realize that people have ideas also. So, it really should be like an operating system kernel. Operating system kernel controls address spaces that can confine computations completely. Therefore you can allow any binaries to come down and be used. You completely control what goes in and what goes out. All of a sudden your browser is simply the thing that allows bitmaps canvases that you give to these things to write on, to be displayed. So, there is nothing you have to do. The good idea in Unix way, hey we do not want a big operating system. We want the tiniest kernel we can have and then we want to use address spaces to protect everybody from everybody else. So this is operating systems 101. This was well known all the way back to 1965 and 1966. It should be the first thing that occurs to anybody. Everybody in the 60s and 70s knew this is the way to do it and we were all surprised it was not done this way. I figured somebody in the current generation would have figured this out, so I went on the Net using Google, looking for downloadable binaries and stuff like that. And I found this. Grad student at Cornell in 1997 Ulfar Erllingson, who is from Iceland, wrote this paper. It is an HTML document that he put on the Net in 1997, and his problem was, hey I want to write a codec, and I do not want for system to be worried about whether it is an executable or not. So this thing should be like an operating system. So of course, I wrote an email to him. He is at the university of Iceland now, but on leave to Microsoft, up in Mountain View, and had a long correspondence with him on the pathway of these ideas. He started a company that Google aquired and is using a couple of the ideas but not all in this paper, which actually solved the problem really nicely. So this is an interesting thing. I was not that somebody did not understand what the problem was and what the right solution was, it is that the larger mass was quite happy to plunge into the defacto standard. Even though most of the people who used the web were computer scientists, they did not protest that this is very bad, non scalable way of doing it. They just plunged in, and 16 years later Google and other companies are struggling to use the Internet the way it was intended to be. 

---

#### Excerpts from [How could the web have been developed following Alan Kay's vision of a browser being a mini-operating system and not just an application?](https://www.quora.com/How-could-the-web-have-been-developed-following-Alan-Kays-vision-of-a-browser-being-a-mini-operating-system-and-not-just-an-application)  

A “real object” is a sandbox — this is one of the many things that are not understood about “real objects”. This is why I use the “OS” metaphor, because OSs confine processes to be sandboxed. The ability to do this at finer grains is made difficult because the CPU makers don’t understand what SW needs. This has led to the very bad idea of unprotected threads running inside the protected processes (what is needed is protection at every level of execution). A good OS is only about resource allocation (space and time and I/O, including display), and should be able to deal with processes without having to know what they are doing. This is what the web browser should do instead of the awful stuff it does do.  

When you were using Squeak you were using a system that gave you the programmer privilege to the metasystem. However, because of what OOP is (with real messaging etc), “fences” can be set up to protect dangerous actions via privilege, etc. One way to look at this is that each meta-level allows more fundamental power but is also more dangerous (even going from pure functions to procedures with side-effects is quite dangerous — even an assignment statement, even inside an object is a meta-level beyond functional relationships).

The point of “messages are not commands” is that something can decide whether to treat a message as a command (in general it shouldn’t). Right now much of the world is reeling because MS didn’t do a good job of this, NSA found out, apparently didn’t tell them right away, got themselves hacked, and now the bug is being exploited because many people didn’t apply the MS patch (and a lot of the reason is because of the pain of dealing with system updates).

Take a look at what your namesake (one of the other Mark Millers) has done with the capability idea. Part of going from “science” and “learning” to “engineering” is to create fences of various kinds. The important idea is that these don’t have to be imposed for all time and early like static languages. But they are needed.

And, yes, the reason we did “Worlds” (and Alex did a great job on this!) was to take a very old very good idea of McCarthy’s — simulating “time” — and try to make it more general and work at more scales. This was proposed for Smalltalk at Parc but it wasn’t pragmatically feasible back then. This is one of many reasons I say that Smalltalk is quite obsolete today — it doesn’t have the stuff that is needed that *can* be done today. Programming language should at least have a “half-life” so they will eventually disappear and force better versions.

---

#### [Exchange with Tony Li](https://www.quora.com/How-could-the-web-have-been-developed-following-Alan-Kays-vision-of-a-browser-being-a-mini-operating-system-and-not-just-an-application)  

**TI**: Imagine the browser instead as a Java virtual machine that you would download code into and execute. Not only could it present a user interface, but it could also do client side computation.

However, this is extremely badly broken from a security standpoint.

**AK**: Well, don’t use Java. Make an OS that can handle and balance intercommunicating fine-grained protected processes in any language.

**TI**: Ok, I’ll bite. Why? The browser was meant to be a ubiquitous display tool, not a means for the server to foist arbitrary computation onto the client.

All of the times when we’ve tried to hand off computation to be executed in a sandbox, we’ve found that the sandbox has holes. It’s a security nightmare.

**AK**: A display for what? If for more than simple static text, the media content could very well require local computation, not just to generate the graphics, but also to get reasonable frame-rates. If we take “computer media” seriously, we have to think about arbitrary computations regardless of how they might be distributed between local and global computation.

The reason I use “OS” as a metaphor and analogy is that mid-modern OSs (starting in the 60s) required some form of hardware confinement to keep processes from messing each other up (this is a real “sandbox”). Most of these are very clunky and intertwined with swapping notions that are now outmoded. However, please do look at the Burroughs B5000 from 1962 to see a fine grained protection and mobility architecture for a multiprocessor machine that was close to uncrashable.

This is why the old original definition of “objects” defined them as virtual computers as encapsulated as HW computers are on the Internet (since you are an Internet guru you will know that a message sent by TCP/IP cannot command a computer in any way — this was intentional in the design). When bad things happen as the result of a TCP message, it is the fault of badly designed SW by someone else (usually an incompetent computerist).

Even with the poor use of gazillions of gates in modern CPUs, it is possible to organize things to protect regions of memory with perfect encapsulation. The offspring of the first “capability” protection offered by the B5000 can be used to make system very secure. One of the main bugs in most systems is that the security is left to the last, rather than being the thing to start with.

**TI**: A very nice dream, but the modern instantiation of hypervisors has proven repeatedly that the walls of the sandbox are way, way too thin. I’d be happy to change my tune if folks were able to demonstrate something robust. I’m not holding my breath.

Until then the responsible amount of computation that can be done on the client has to be exceedingly limited and undoubtedly inadequate by your standards. Sorry to say this, but we’ve blown off security for far too long and it’s now come back to bite us. Until we fix it, we won’t be making significant forward progress.

#### [Exchange with Miles Fidelman](https://www.quora.com/How-could-the-web-have-been-developed-following-Alan-Kays-vision-of-a-browser-being-a-mini-operating-system-and-not-just-an-application)  

**MF**: HyperCard almost got us there. So did OpenDoc.

Then again, things get very complicated, very quickly. Ted Nelson’s original vision of hyperlinks was bi-directional - nice in theory (much harder for links to break) - but much harder in practice (a static reference is lot easier to implement).

I expect things would get very, very messy if every web page contained tons of actors - all communicating continuously across the net. The potential for deadly embraces, resource exhaustion, and all kinds of nasty effects is pretty huge (not to mention whole new classes of spam, viruses, DDoS attacks, and so forth).

(Mind you, the notion of web pages as “smart documents” is intriguing - and an area where I’ve done some funded r&d - and looking for more. It’s just not that easy. Object Linking and Embedding, across Microsoft Office documents is bad enough. Making that work across the Internet, interoperably, is a long way off.)  

**AK**: Actually, not really. Take a look at the Croquet system (a realization of David Reed’s 1978 MIT Thesis), and its current manifestation as Open Cobalt (both of these have entries in Wikipedia). The idea of “simulating time” that goes back to John McCarthy allows much more than the traditional approaches.

I think the biggest problem here is that most computerists put forth enormous efforts roughly at the first levels of learning that they’ve gotten to. This is equivalent to trying to do science with what the Alexandrian Greeks were able to do — impressive compared with no science, but not when compared to the 17th century onward, and especially the 20th and the 21st centuries.

Along with all the rather ad hoc poking around that has characterized our field, the exceptions that are “mathematical and scientific” have allowed very difficult things (like the Internet (not the web)) to be designed and work well.    

**MF**: Hi Alan!

It’s your vision and interview being cited, so yours would be the definitive answer. :-) But…

From a technical perspective, I should think that networked, hyperlinked, Hypercard stacks (or OpenDoc containers) look a lot more like combining browser, operating system, and an object-oriented environment into one, than Croquet. (Though, perhaps your vision was not of a “browser being a mini-operating system and not just an application” as represented by the OP.)

Now Croquet has a coolness factor, and Teatime is an interesting protocol - but then immersive environments have limited utility for most things, and DIS & HLA are a bit more mature for maintaining distributed environments. But that’s another discussion entirely.  

**AK**: Many people have been distracted by the immersive shared 3D environment that is one of the things built on top of the Reed ideas, but if you just forget about this for a second and think about what the underlying system is doing, you’ll start to see the significance of the ideas. It’s basically internet-wide transactions on distributed objects.

When some of us were at Apple, we urged the company to use Hypercard as a model for end-user symmetric authoring and using. But these are different — though important — compared to what needs to be done at the systems level, and in particular to allow objects to freely be mixed to make media.  

**MF**: Now that’s a good point. It is a bit of a shame that the focus remained on the 3D environment. It does seem a bit of a shame that nobody ever pushed Teatime as a protocol, complete with RFC, multiple implementations, etc.

Then again, for whatever reason, this seems like an idea that’s never gotten traction outside of specialized arenas. Sun pushed “distributed objects everwhere,” Taligent never got anywhere, etc. DIS & HLA are pretty mature at this point, which do deal with distributed objects - and both are standardized - but no real traction outside the military training community (you’d think that HLA would be a great platform for MMORPGs).

Maybe it’s time for another push.  

**AK**: Dave Reed’s (and Croquet’s) schemes are much simpler and quite a bit more general: they are about the larger idea of “single logical objects” being able to manifest as “complete ghosts” on as many computers as needed over large slow networks. This is like atomic transactions on distributed databases but for objects and much finer grained in both space and time.  

And you are right that addressing these issues is very long overdue.

**MF**: Well, that is what DIS & HLA do - they’re all about synchronized replicated world models (every simulator has it’s own copy of the world). But I’m going to have to revisit Teatime in relation to some work I’m currently doing on replicated/synchronized documents.  

**AK**: Take a look at Reed’s 1978 MIT thesis for the gist.

**MF**: Thanks! Actually, I have read the papers, and a lot of documentation. (And talked with David about it, on occaision :-) I do need to refresh myself, though.

---

#### Excerpts from [What are the difficulties of creating "smart APIs", where two programs can automatically figure out a way to talk to each other?](https://www.quora.com/What-are-the-difficulties-of-creating-smart-APIs-where-two-programs-can-automatically-figure-out-a-way-to-talk-to-each-other)   

Also within this community [ARPA/PARC] were ideas about "software computers connected by neutral messaging" that could be a universal scalable way to unify -- and make mobile -- software systems, even with heterogeneous software systems (using similar encapsulation ideas and separating protocols from methods) that could freely be mapped onto the interconnected computers that the Internet would make possible. For example, Smalltalk at Xerox Parc in the 70s experimented with "proxy objects" which were gateways to objects in other systems and computers.

The really good idea of "the Internet as a transparent cache for "intercommunicating object-processes" never gained wide popularity (but see Reed's thesis below).

Also at Xerox Parc during the same period, it was realized that it was easier to send a program to a server to be "executed in confinement" than to try to get all servers to understand all data formats (this is where Postscript emerged from a graphics making language to a "traveling communications protocol"). The insight is that a rather simple executable language can be made universal, but that there is no end to possible formats of "data". This fits with the overall reasons for "send objects, not data".

Of great note is the [1978 MIT PhD thesis by Dave Reed](http://publications.csail.mit.edu/lcs/pubs/pdf/MIT-LCS-TR-205.pdf) -- who is also known as "the 'slash' in TCP/IP" -- which sketches an architecture for an Internet-wide operating system of coordinated objects running in a common pseudo-time. (These ideas were later validated in the [Croquet](https://www.researchgate.net/publication/4029258_Croquet_-_A_collaboration_system_architecture) System ca 2003 by Smith, Raab, Reed, and Kay).

The first great practical demonstration of "dynamic mobile processes" across heterogeneous hardware was done in ["The LOCUS Operating System"](https://mitpress.mit.edu/books/locus-distributed-system-architecture) by Gerry Popek and his group at UCLA during the early 80s. This wasn't extended beyond underlying Unix processes, but could have been. The first several chapters of the MIT Press book are excellent in laying out the general issues for machine independent processes.

There are also some "unfortunate" stabs at universal interfaces that pretty much missed the point of scaling, the Internet, and heterogeneity of software -- for example: [CORBA](https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture).

---

#### Excerpts from [Alan Kay has agreed to do an AMA today](https://news.ycombinator.com/item?id=11948686)

**AK**: It's worth thinking about what scales and what doesn't scale so well. For example, names are relatively local conventions. We could expect to have to find better ways to describe resources, or perhaps "send processes rather than messages". Think about what's really interesting about the way Parc used what became Postscript instead of trying to define a file format for "documents" for printers ... (a programming language can have far few conventions and be more powerful, so ...)

**Hacker**: Thanks, Alan. I guess sometimes is hard to think about that level of scaling when working on the industry... or at least in projects that are not that massive.
I assume your are talking in the lines of "call by meaning" when you mention that names are relatively local, right?
As for "send processes rather than messages", isn't that what objects are about?
I mean...sending the real thing, not just "data" as part of a message. That reminds me of the Burroughs 220 and "delivering video + codec together" example you mention in your talks.

**AK**: Modularity is: what is the minimum you need to know to make use of a module, and what is the minimum that a module needs to know to help you?
Making this good is one of the secrets of scaling

**Hacker**: Im afraid I'm missing the point about "sending processes rather than messages".
The modularity thing sounds pretty much to well designed objects to me, but it seems that you're trying to make a difference between that and processes.
What do you have in mind or, better said, which could be a concrete example of it?

**AK**: The question is whether a "message" has enough "stuff" to reify into a real process (so it can help interpretation and negotiation) or whether the receiver has to do all the work (and thus perhaps has to know too much for graceful scaling of the system).

#### Excerpt from [Enterprise Integration Patterns: Designing, Building, and Deploying Messaging Solutions](https://www.amazon.com/o/asin/0321200683/ref=nosim/enterpriseint-20); page 33

Also, existing XML Web Services standards address only a fraction of the integration challenges. For example, the frequent claim that XML is the ‘Lingua franca” of system integration is somewhat misleading. Standardizing all data exchange to XML can be likened to writing all documents using a common alphabet, such as the Roman alphabet. Even though the alphabet is common, it is still being used to represent many languages and dialects, which cannot be readily understood by all readers. The same is true in enterprise integration. The existence of a common presentation (e.g. XML) does not imply common semantics. The notion of “account” can have many different semantics, connotations, constraints and assumptions in each participating system. Resolving semantic differences between systems proves to be a particularly difficult and time-consuming task because it involves significant business and technical decisions.

#### [Alan Kay at MIT-EECS 1998 Fall Semester Colloquium Series](https://www.youtube.com/watch?v=BUud1gcbS9k&t=1h26m30s) 

My theory about security is very similar to locks, in that you can create greate locks, but somebody with enough resources can get through the lock. Basically, I think locks are for is to discourage various grades of criminal. In particular, these things I was showing as projects, these kind of desktops, are actually independent modules, because they are designed as things that you might wander into on the Internet. They have certain security to them. Then there is an interesting question how you run them safely when you bring them into your machine. Maybe the only thing I can add to mechanisms that everybody are familiar with, because there any number of ways of doing things, including ... we ran at Apple successfully for many years was simply confining ... making it impossible for the system to write to the disk in unrestricted ways ... confining other kinds of things. I have a prejudice about how should be done, which is orthogonal but synergistic with encription ideas. It is basically that, on every machine, although Apple does not use it, there is memory management unit, and it is not used very well on Windows either. If you think about the general problem of dealing with the Internet ... you are out there, you see something which looks like it might be useful ... let's say it is an object ... the probability that it is going to be your favorite kind of object, like a Squeak object is low, or any other kind of object. But, if it is really an object, it should communicate with other kinds of objects not of it's species. Otherwise, what use is having an object. You really do not have an object if they cannot send messages to ... if the Squeak object cannot send messages to a Java object, it is not very object-oriented, and neither is Java. So if you get this notion, that objects are kind of recapitulating all the heterogeneity of hardware on the network and using TCP/IP for sending messages back and forth. Then, the question is, if I have to bring an object in, in order to run it in a reasonable way, how should I do it? I believe the answer is, you stick that object in a separate address space. And if it needs to bring in it's handle, you let it bring it's handler into that separate address space, and you run that object or any other object in that special address space. And you monitor the traffic between that address space and other parts of the system. That is the way I would like to do it. And you can encrypt wherever you want. But to me what you would really want to do is to be able to enforce what the set of things you are not quite sure are, can do, but still let them run. This is what you might call a constitutional approach. We do not want to tell objects what to do, but you want to make sure they cannot do a lot of damage. 

#### [A Conversation with CMU Faculty & Students](http://www.youtube.com/watch?v=PFc379hu--8&t=46m33s)  
The last time I looked at the task manager on my Windows OS I only saw 30 processes running. I would expect to see ... your little laptop there is equivalent of the entire Internet of the 80s sometime (computing, storage, processing power). When I look inside it I would expect to see a few thousand objects with no center, not tied to any particular operating system (why should they be?). Lets take Internet seriously for just one second. I love talking about it cause I almost have nothing to do with it. I have something to do with ARPANET, but the Internet was done by colleagues of ours, so it is not like claiming that Smalltalk is better than Java, which would be a bad thing to do. Since I have nothing to do with it I can praise it to the skies. Lets just look at it abstractly for a second. It is maybe the only real object system on Earth, where each object is entire computer (there is nothing more encapsulated than a piece of hardware), and pure messages are sent to it, really pure. One of the most interesting things about the Internet is that if an object that receives a message crashes as a result of that message, it is it's own fault, because we are not sending 100 thousand volts over the line there, we are just sending bits. Those bits are entirely interpreted by the programs on that object and the object gets to decide what to do. Suppose we can do that efficiently in smaller things (these are the thoughts I had in 1966). So the thing I love about the Internet is ... if I need something, there might be some object outhere that can supply it and it might be able to be of service to me, I do not care about the operating system it is running on or any other thing, because it is a pure messaging system. So, I am maybe doing one kind of thing, and that guy over there is doing maybe some other thing, I do not care. Next thing I want to do is to somehow get all those objects into my machine. So, the first thing I would talk to our friends at Intel about is why do not you understand virtual machines. The most important thing you can do on the computer today is confinment. So lets use some of that memory that you are using badly by doing caching wrong. Lets use it so that we can have thousands and thousands of virtual address spaces completely inexpensive. If you do math on it, it is absolutely possible. It is just not architecturaly in Intel's mind. Nobody had really explained to them why is this a good idea. And why do we not make interprocess messaging really fast, instead really slow the way Intel does it? And all of a sudden you start thinking ... I do not need an operating system, because what I really want are facilities (services). I must have confinment because things can be dangerous, but I do not want to rely on software for that - I want real confinment. I want to be able to get anything from Timbuktu that looks reasonable and chuck it into a virtual address space and confine the shit out of it. I want to do experiments with that thing. I want to feed it things and see what it feeds back to me. I want to use it in various guarded ways, etc. These are not new ideas, but they are like magic if you try to explain it to Intel, because Intel could not even to get to the point of allowing programmers to microcode so you can do higher-level languages. Level of retrograde here is really back before 1965, but it helps to be in computing before 1965 to see it, cause today for most people it looks normal.

#### [Richard Kelsey at Dynamic Languages Panel (on runtime)](http://www.youtube.com/watch?v=SjbtEnfm7_Q&t=46m00s)
Good idea that has come back is time-sharing. It used to be that computers are big, expensive things and because of that you have to share them with other people. Whole a lot of work went into setting them up so that people can share them without stepping on each other too much and getting into too much trouble. Then computers got very cheap, everybody had their own and nobody paid attention to that anymore. On these computer, esentially put any program you want and it can do whatever it wants. Joke: "Reformatting a hard drive is a right. Programs need to be able to do that." Computers are nowdays so cheap that you are the only person sitting on the computer, but that computer is probably connected to the network, and if not that, it is running a lot of programs written buy other people. So you start something, and then it starts something else, etc. So it has turned back into a multiuser machine. We need to go back to dealing with it as a multiuser machine, so that different programs can run and not step on each other. I am not suggesting to just go back and take ITS and put it on the machine nowdays (Scott Mckay comment: this is not a bad idea). These programs need to be able to run with each other, protect themselves from each other. Operating systems today are not going to help very much. They either did not heard about protection, or they use protection from 1960. It really turns into language level issue. I think you can solve it from a language level using techniques that have been around for a long time, but are coming back into use. One that I have been looking into recently are capabilities which in area I work in comes to "Lambda - the ultimate security tool". 

#### [Internet all the way down](http://www.youtube.com/watch?v=tp9VbtLn2Jw&t=27m26s)  
Everything is kind of like the Internet. The networks themselves were distributed, whether it is Internet or Ethernet. Inside of computers there was no OS, it was what you might think as "Internet all the way down". It is interesting to think about ... a machine like this in which, if you calculate it out, it has about the capacity and computing power of the entire Internet in the 80s (Moore's Law). So you would expect, if you look inside you would find something that has thousands of virtual machines and dozens of physical processors, acting very much like your own local Internet, in fact being a cache for the processes running on the Internet - that is kind of the way we thought of it back then. But instead what you find is something from the 60s, called an operating system, that esentially stove pipes most things in a very, very bad way, makes it very difficult to integrate things, whereas it is very easy to integrate on the Internet.

#### [The Deep Insights of Alan Kay - On Messaging](http://mythz.servicestack.net/blog/2013/02/27/the-deep-insights-of-alan-kay/)  
To me, one of the nice things about the semantics of real objects is that they are “real computers all the way down (RCATWD)” – this always retains the full ability to represent anything. The old way quickly gets to two things that aren’t computers – data and procedures – and all of a sudden the ability to defer optimizations and particular decisions in favour of behaviours has been lost. In other words, always having real objects always retains the ability to simulate anything you want, and to send it around the planet. … And RCATWD also provides perfect protection in both directions. We can see this in the hardware model of the Internet (possibly the only real object-oriented system in working order). You get language extensibility almost for free by simply agreeing on conventions for the message forms. My thought in the 70s was that the Internet we were all working on alongside personal computing was a really good scalable design, and that we should make a virtual internet of virtual machines that could be cached by the hardware machines. It’s really too bad that this didn’t happen. If ‘real objects’ are RCATWD, then each object could be implemented using the programming language most appropriate for its intrinsic nature, which would give new meaning to the phrase ‘polyglot programming.’

The people who liked objects as non-data were smaller in number, and included myself, Carl Hewitt, Dave Reed and a few others – pretty much all of this group were from the ARPA community and were involved in one way or another with the design of ARPAnet->Internet in which the basic unit of computation was a whole computer. But just to show how stubbornly an idea can hang on, all through the seventies and eighties, there were many people who tried to get by with “Remote Procedure Call” instead of thinking about objects and messages. Sic transit gloria mundi.

#### [Croquet System](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.164&rep=rep1&type=pdf)

#### [Comments about browsers at Dynamic Languages Wizards Series - Panel on Language Design](http://www.youtube.com/watch?v=agw-wlHGi0E&t=1h54m00s)  
Paul Graham: All I would ever dare assume is that you can show HTML pages and forms work, and even forms might not necessarily work. I certainly would not assume that JavaScript works. It seems like people who write software that depends on JavaScript are always 85% done.  
Guy Steele: HTML is ubiquitous, and at least once a day I download some page that crashes my browswer, and switching to the other browser does not help always.  

#### Questions
1. Does Actor model (and specifically one in Akka) allow sending objects (processes/code), not just data?
