
Related resources:

[Is a microservices architecture with RESTful APIs an implementation of Alan Kay's concept of object-oriented programming?](https://www.quora.com/Is-a-microservices-architecture-with-RESTful-APIs-an-implementation-of-Alan-Kays-concept-of-object-oriented-programming)  
[What are the difficulties of creating "smart APIs", where two programs can automatically figure out a way to talk to each other?](https://www.quora.com/What-are-the-difficulties-of-creating-smart-APIs-where-two-programs-can-automatically-figure-out-a-way-to-talk-to-each-other) 
[How could the web have been developed following Alan Kay's vision of a browser being a mini-operating system and not just an application?](https://www.quora.com/How-could-the-web-have-been-developed-following-Alan-Kays-vision-of-a-browser-being-a-mini-operating-system-and-not-just-an-application)  

#### [Seminar on Object Oriented Programming](https://www.youtube.com/watch?v=QjJaFG63Hlo); starts at 2:11:50
The original Smalltalk did not use kind of procedure call ... it really acted as you were on a network. ... When you run Smalltalk over a network people make connection to the network by having what are called phantom objects. Phantom object is a stand-in for the object you are sending message to ... cause you are always sending a message to something. .. [Further explanation is done whith [this](slides/sending-object.png) slide] ... Suppose you are here and you think you are sending a message from object A to object B. They are maybe on different machines, so what is maybe really going on is object A is sending to a version of object B which is really going over a network to real B. These are called phantom objects. There is usually one class of them, and whenever you are distributing computation, usually the environment for objects of few local objects and the rest of them are phantom objects. Phantom objects take care of all of the network stuff and synchronizations, and reorderings and other kinds of things you have to do when you are going to slower media. That is well established technique, and in any good object-oriented language you can set that up literary in an afternoon. It is quite instructive to think about what it means not to send just a message but to send an object from one place to another. What parts of it's environment we have to bring along with it in order for it to be understood? In theory, sending an object (say we send object C across) ... C might not be well understood on the other side. On the other hand, it is still active, so if your network has the rule, as ours did at PARC, that whenever an object shows up and it's class is not there, the clone of it's class gets sucked over with it. Compare that to sending a data structure, which is sending bits, and how fragile they are, because you are not sure there are procedures on the other end to know them. Here you are ensuring that either nothing at all happens or right procedures are automatically sent along. That is exactly how Air training command protected their tapes [reference to the story from the beginning of the presentation]. You just ensure that procedures goes along with them. And it is quite easy, using these phantom objects to make that automatic. So you can think of the network objects as constantly rebalancing themselves as to where the code actually is. 

#### [The computer revolution hasnt happened yet - OOPSLA 1997](https://www.youtube.com/watch?v=oKg1hTOQXoY) 21:24 - 26:05  
This is the earliest known form of what we call data abstraction. It goes all the way back to pre '61. I was in the Air Force in 1961 and I saw it then. It probably goes back one year before. Back then they did not really have operating systems. Air training command had to send tapes of many kinds of records around from air force base to air force base. There was a question how could you deal with all these that used to be card images, and then because tape had come in there are starting to be more and more complicated formats. And somebody, almost cerainly an enlisted man, because officers did not program back then, came up with the following [idea](slides/b220.png). This person said, on the third part of the record on this tape we will put all the records of this particular type. On the second part we will put all the procedures that know how to deal with the formats on this third part of the tape. And on the first part we are going to put pointers into the procedures and lets make first 10 or so pointers standard, like reading and writing fields, trying to print, lets have standard vocabulary for the first 10 of these and then we can have idiosyncratic ones later on. So all you have to do to read a tape back in 1961 is to read the front part of the record in core storage and start jumping indirect through the pointers and the procedures were there. Now, I would really like you to contrast that with what you have to do with HTML on the Internet. Think about it. HTML on the Internet has gone back to the dark ages because it presupposes that there should be a browser that should understand it's formats. That has to be one of the worse ideas since MS-DOS. This is really a shame. This is maybe what happens when physicists decide to play with computers, I am not sure. And in fact, we can see what has happened to the Internet now. There are 2 wars going on. There is set of browser wars which are 100% irrelevant. They are basically an attempt either at demonstrating a non-understanding of how to build complex systems, or even cruder attempt simply to gather teritory. I suspect Microsoft is in the latter camp here. You do not need a browser if you followed what this staff sergeant in the Air Force knew how to do in 1961. You just read it in, it should travel with all the things it needs, and you do not need anything more complex than something like X-Windows, hopefully better. But, basically you want to be able to distribute all of the knowledge of these things that are there. And in fact the Internet is starting to move in that direction as people discover ever more complex HTML formats ever more intractable. This is one of these mistakes that has been recapitulated every generation, and it is just simply not the way to do it. So, here it is a great idea. BTW, this kind of programming was done before there were higher level languages in the Air Force, and this approach to things was forced out of the Air Force by Cobol, when the standardized on Cobol.  

#### [What if "data" is a really bad idea? - excerpt of exchange between Alan Kay and Rich Hickey](https://news.ycombinator.com/item?id=11945722)  

**RH:** Data like that sentence? Or all of the other sentences in this chat? I find 'data' hard to consider a bad idea in and of itself, i.e. if data == information, records of things known/uttered at a point in time. Could you talk more about data being a bad idea?

**AK:**  What is "data" without an interpreter (and when we send "data" somewhere, how can we send it so its meaning is preserved?) 

**RH:** Data without an interpreter is certainly subject to (multiple) interpretation :) For instance, the implications of your sentence weren't clear to me, in spite of it being in English (evidently, not indicated otherwise). Some metadata indicated to me that you said it (should I trust that?), and when. But these seem to be questions of quality of representation/conveyance/provenance (agreed, important) rather than critiques of data as an idea. Yes, there is a notion of sufficiency ('42' isn't data).

Data is an old and fundamental idea. Machine interpretation of un- or under-structured data is fueling a ton of utility for society. None of the inputs to our sensory systems are accompanied by explanations of their meaning. Data - something given, seems the raw material of pretty much everything else interesting, and interpreters are secondary, and perhaps essentially, varied.

**AK:** There are lots of "old and fundamental" ideas that are not good anymore, if they ever were.

The point here is that you were able to find the interpreter of the sentence and ask a question, but the two were still separated. For important negotiations we don't send telegrams, we send ambassadors.

This is what objects are all about, and it continues to be amazing to me that the real necessities and practical necessities are still not at all understood. Bundling an interpreter for messages doesn't prevent the message from being submitted for other possible interpretations, but there simply has to be a process that can extract signal from noise.

This is particularly germane to your last paragraph. Please think especially hard about what you are taking for granted in your last sentence.

**RH:** Without the 'idea' of data we couldn't even have a conversation about what interpreters interpret. How could it be a "really bad" idea? Data needn't be accompanied by an interpreter. I'm not saying that interpreters are unimportant/uninteresting, but they are separate. Nor have I said or implied that data is inherently meaningful.

Take a stream of data from a seismometer. The seismometer might just record a stream of numbers. It might put them on a disk. Completely separate from that, some person or process, given the numbers and the provenance alone (these numbers are from a seismometer), might declare "there is an earthquake coming". But no object sent an "earthquake coming" "message". The seismometer doesn't "know" an earthquake is coming (nor does the earth, the source of the 'messages' it records), so it can't send a "message" incorporating that "meaning". There is no negotiation or direct connection between the source and the interpretation.

We will soon be drowning in a world of IoT sensors sending context-or-provenance-tagged but otherwise semantic-free data (necessarily, due to constraints, without accompanying interpreters) whose implications will only be determined by downstream statistical processing, aggregation etc, not semantic-rich messaging.

If you meant to convey "data alone makes for weak messages/ambassadors", well ok. But richer messages will just bottom out at more data (context metadata, semantic tagging, all more data) Ditto, as someone else said, any accompanying interpreter (e.g. bytecode? - more data needing interpretation/execution). Data remains a perfectly useful and more fundamental idea than "message". In any case, I thought we were talking about data, not objects. I don't think there is a conflict between these ideas.

**AK:** 2nd Paragraph: How do they know they are even bits? How do they know the bits are supposed to be numbers? What kind of numbers? Relating to what?

Etc

**RH:** It contravenes the common and historical use of the word 'data' to imply undifferentiated bits/scribbles. It means facts/observations/measurements/information and you must at least grant it sufficient formatting and metadata to satisfy that definition. The fact that most data requires some human involvement for interpretation (e.g. pointing the right program at the right data) in no way negates its utility (we've learned a lot about the universe by recording data and analyzing it over the centuries), even though it may be insufficient for some bootstrapping system you envision.

#### [Normal Considered Harmful](https://www.youtube.com/watch?v=FvmTSpJU-Xc) 16:02 - 27:00
Some years ago my wife ... she had one of these huge Mac Cinema displays on her desk, and she showed me this display. It is kind of odd. Over here she had all the applications that she used were WYSIWYG. She really like them. Here are all the stuff she does on the web and none of it was WYSIWYG at that time. In these apps I can see and do full WYSIWYG authoring. Web broswer has all these modes, I have to type throught a keyhole, I have to wait to see whether what I did is ok. She asked me, why is that? I said, because the stuff she liked way invented in the 70s and the stuff she does not like was invented 20 years later. What do you think about that? How could that happen? ...
